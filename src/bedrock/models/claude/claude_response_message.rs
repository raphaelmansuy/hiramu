use serde::{Deserialize, Serialize};

/// Represents the response from the Claude API.
#[derive(Serialize, Deserialize, Debug)]
pub struct ResponseMessage {
    /// The unique identifier for the response.
    pub id: String,
    /// The ID for the Anthropic Claude model that made the request.
    pub model: String,
    /// The reason why Anthropic Claude stopped generating the response.
    pub stop_reason: StopReason,
    /// The type of response. The value is always "message".
    pub r#type: String,
    /// The conversational role of the generated message. The value is always "assistant".
    pub role: String,
    /// The content generated by the model. Returned as an array.
    pub content: Vec<ContentBlock>,
    /// Container for the number of tokens that you supplied in the request and the number of tokens that the model generated in the response.
    pub usage: Usage,
    /// The model generated one of the stop sequences that you specified in the stop_sequences input field.
    pub stop_sequence: Option<String>,
}

/// Represents the reason why Anthropic Claude stopped generating the response.
#[derive(Serialize, Deserialize, Debug)]
#[serde(rename_all = "snake_case")]
pub enum StopReason {
    /// The model reached a natural stopping point.
    EndTurn,
    /// The generated text exceeded the value of the max_tokens input field or exceeded the maximum number of tokens that the model supports.
    MaxTokens,
    /// The model generated one of the stop sequences that you specified in the stop_sequences input field.
    StopSequence,
}

/// Represents a content block within the response.
#[derive(Serialize, Deserialize, Debug)]
pub struct ContentBlock {
    /// The type of the content block. Currently, the only supported value is "text".
    pub r#type: String,
    /// The text of the content.
    pub text: String,
}

/// Represents the usage information in the response.
#[derive(Serialize, Deserialize, Debug)]
pub struct Usage {
    /// The number of input tokens in the request.
    pub input_tokens: u32,
    /// The number of tokens that the model generated in the response.
    pub output_tokens: u32,
}